---
title: "R for statistics"
author: "Phillip Hamrick, Ph.D., PI Language and Cognition Research Laboratory"
date: "8/19/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---


## Correlations
A common way for people to do research is to look for quantitative relationships between variables. For example, in my research I often look for quantitative relationships between bilingual lexical abilities and their declarative memory abilities. 
A common first step in doing these sorts of analyses is to use a correlation test. The most common ones are Pearson's r (for normally distributed intervalic/continuous data), Spearman's rho (for ordinal or rank data), and Kendall's tau (for categorical data).

```{r Correlations}
# First, clear the work environment
rm(list = ls())

# Now, load the correlations data worksheet
d <-read.csv("correlation.csv")

# We have three variables, self-rated motivation, vocabulary proficiency score, and hair length
# Let's convert them to their proper numerical values
d$motivation <-as.numeric(d$motivation)
d$vocabulary <-as.numeric(d$vocabulary)
d$hair_length <-as.numeric(d$hair_length)

# To run a correlation, we can either correlate two specific variables or correlate everything in the document with each other. Let's try both.
cor.test(d$motivation, d$vocabulary) # correlates two variables; r = .47, p = .02
cor(d) # produces a matrix table of the correlations between all variables

# We might also want both correlation coefficients AND p-values, which we can do with the Hmisc package
library(Hmisc)
rcorr(as.matrix(d), type = "pearson")

# As expected, in our hypothetical data set, motivation and vocabulary are correlated positively, whereas hair length has no substantial correlation with either motivation or vocabulary.

# We know from our reading that we can square the R value to give us shared variance or explained variance. Now we're just back to basic math, keeping in mind that R is a calculator!
R2 = .47^2 # R2 = .22

# We can graph a correlation using a scatterplot. For now, just run the code, and we can worry more about graphing later.
ggplot(d, aes(x = motivation, y = vocabulary)) + geom_point() + geom_smooth(method = "lm")
# note: some data points fit the line well, others less so,  but there is a general upward trend.
# note: the dark grey line represents the 95% confidence interval of the correlation line.

```
OK. So now you can do a basic correlation! That's it! If you have data that require using Spearman's rho or Kendall's tau, then you add that to your code as "method = N". For example, cor.test(d$motivation, d$vocabulary, method = "kendall").



## Regression modeling
The idea of regression modeling is essentially this: use a series of predictor variables to predict an outcome variable. For example, we can use current weather conditions along with the previous reported weather from years past to predict what the weather will be like tomorrow. Similar to correlations, regression modeling can allow us to try to predict bilingual proficiency by looking at bilingual motivation. Similar to ANOVA, regression modeling can allow us to try to predict vocabulary scores based on teaching method used (A, B, or C). More than that, we can have MULTIPLE predictor variables simultaneously so that we can try to, for example, predict vocabulary scores based on teaching method used (A, B, or C), while also taking into consideration learners' motivation levels. 
  
```{r Regression modeling}
# To keep things simple to start, let's look at our repeated measures ANOVA dataset again.

rm(list = ls())

d <-read.csv("pre-post-delayed.csv")
d$test <-as.factor(d$test)

# Now we can build a regression model. The idea is the same as before. We have an outcome variable (test score) and a predictor variable (test type). When I build a model, I like to give it a name. I use m (meaning model) followed by a number (e.g., 1) in case I'm building multiple models.

m1 <-lm(score ~ test, data = d)
summary(m1)


# OK, now let's try another, but this time with multiple predictor variables

rm(list = ls())
d <-read.csv("Stats Workbook.csv")

# There's a lot of data here; let's try to model Vocabulary Score at Level 2 proficiency based on learners' sex and their working memory abilities.
d$Sex <-as.factor(d$Sex)
d$Working.Memory <-as.numeric(d$Working.Memory)

m1 <-lm(Vocabulary.Score..Level.2 ~ Sex + Working.Memory, data = d); summary(m1)

# Cool. It looks like working memory makes a difference, but sex does not. But what if the working memory effect was different for males and females? To look at that, we need to add an interaction term using the asterisk or multiply symbol.

m2 <-lm(Vocabulary.Score..Level.2 ~ Sex*Working.Memory, data = d)
summary(m2)

# Great. So it looks like better working memory predicts better vocabulary scores, but not in a statistically significantly different way between males and females. Also, note that adjusted R-square for m2 is smaller than for m1. That means that model m1 is both more parsimonious (simpler) and explains more of the variance in the data, and those are both desirable outcomes!
```


## Graphing
R is great for a lot of things, but perhaps my favorite is it's ability to produce killer graphs. For a handy cheat sheet for making graphs with ggplot2(), I refer you to this link: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf

Let's try making the two most important kind of graphs you'll want to produce in your research: boxplots (for mean differences and regression with only categorical predictors) and scatterplots (for correlations and regression with continuous predictors)


```{r Graphing}
rm(list = ls())

# Let's look at a boxplot of our repeated measures ANOVA data first.
d <-read.csv("pre-post-delayed.csv")
d$test <-as.factor(d$test)

ggplot(d, aes(x = test, y = score)) + geom_boxplot() # simple boxplot


# now let's add color and some nicer labels and make sure the y-axis represents the full range of possible scores
ggplot(d, aes(x = test, y = score, color = test)) + geom_boxplot() + xlab("Test Type") + ylab("Test Score %") + ylim(0, 100)


# much better, but we might want to put these in a different order to represent the flow of time from pre- to post- to delayed-post test.
d$test <-factor(d$test, levels = c('pre', 'post', 'delayed', order = TRUE))


# now re-run the graph code for a nice graph
ggplot(d, aes(x = test, y = score, color = test)) + geom_boxplot() + xlab("Test Type") + ylab("Test Score %") + ylim(0, 100)



# Let's look at a scatterplot of our regression data, and let's add in the sex variable just for some practice with more complex graphs
rm(list = ls())
d <-read.csv("Stats Workbook.csv")
d$Working.Memory <-as.numeric(d$Working.Memory)
d$Sex <-as.factor(d$Sex)

# facet wrap sex
ggplot(d, aes(x = Working.Memory, y = Vocabulary.Score..Level.2)) + geom_point() + geom_smooth(method = "lm") + xlab("Working Memory Score") + ylab("Vocabulary Score at Level 2") + facet_wrap(~ Sex)

# color sex
ggplot(d, aes(x = Working.Memory, y = Vocabulary.Score..Level.2, fill = Sex, color = Sex)) + geom_point() + geom_smooth(method = "lm") + xlab("Working Memory Score") + ylab("Vocabulary Score at Level 2")

```


